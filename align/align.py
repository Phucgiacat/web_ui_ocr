import pandas as pd
import numpy as np
import ast
import os
from pathlib import Path
from .nom_process import process_nom
from .vi_process import process_quoc_ngu
from tqdm import tqdm
from dotenv import load_dotenv
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env", override=True)

def build_dicts(similar_df, trans_df):
    trans_dict = {}
    for _, row in trans_df.iterrows():
        word, han_char = row.iloc[0], row.iloc[1]
        trans_dict.setdefault(word, []).append(han_char)

    similar_dict = {}
    for _, row in similar_df.iterrows():
        char, sim_char = row.iloc[0], row.iloc[1]
        similar_dict.setdefault(char, []).append(sim_char)

    return trans_dict, similar_dict

def is_compatible(han_nom_char, quoc_ngu_word, trans_dict, similar_dict):
    hn_candidates = trans_dict.get(quoc_ngu_word, [])
    similar_chars = similar_dict.get(han_nom_char, []) + [han_nom_char]
    return bool(set(hn_candidates) & set(similar_chars))

def levenshtein_align_boxes(nom_list, qn_list, similar_df, trans_df):
    trans_dict, similar_dict = build_dicts(similar_df, trans_df)
    m, n = len(nom_list), len(qn_list)
    dp = np.zeros((m + 1, n + 1), dtype=int)
    backtrace = np.full((m + 1, n + 1), '', dtype=object)

    for i in range(m + 1):
        dp[i][0] = i
        backtrace[i][0] = 'U'
    for j in range(n + 1):
        dp[0][j] = j
        backtrace[0][j] = 'L'
    backtrace[0][0] = ''

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            match = is_compatible(nom_list[i - 1], qn_list[j - 1], trans_dict, similar_dict)
            cost = 0 if match else 1
            options = [
                (dp[i - 1][j] + 1, 'U'),
                (dp[i][j - 1] + 1, 'L'),
                (dp[i - 1][j - 1] + cost, 'D')
            ]
            dp[i][j], backtrace[i][j] = min(options)

    aligned_nom, aligned_qn = [], []
    i, j = m, n
    while i > 0 or j > 0:
        if i > 0 and j > 0 and backtrace[i][j] == 'D':
            aligned_nom.append(nom_list[i - 1])
            aligned_qn.append(qn_list[j - 1])
            i -= 1
            j -= 1
        elif i > 0 and backtrace[i][j] == 'U':
            aligned_nom.append(nom_list[i - 1])
            aligned_qn.append("*")
            i -= 1
        elif j > 0 and backtrace[i][j] == 'L':
            aligned_nom.append("*")
            aligned_qn.append(qn_list[j - 1])
            j -= 1

    aligned_nom.reverse()
    aligned_qn.reverse()
    return [aligned_nom, aligned_qn]

def align(nom_dir, vi_dir, output_txt, k=1, name_book="book", reverse=False, mapping_path=None):
    similar = pd.read_excel(os.environ['NOM_SIMILARITY_DICTIONARY'])
    trans = pd.read_excel(os.environ['QN2NOM_DICTIONARY']).iloc[:, [0, 1]]
    
    # X√≥a file output c≈© n·∫øu c√≥
    if os.path.exists(output_txt):
        os.remove(output_txt)
    
    # Extract first name and last number from filename for sorting
    def extract_name_and_last_number(filename):
        """Extract first name part and last number from filename.
        e.g., phuc_001_002_001.json -> ('phuc', 1)
        """
        name_without_ext = os.path.splitext(filename)[0]
        parts = name_without_ext.split("_")
        
        # Get first part as name
        first_name = parts[0] if parts else ""
        
        # Get last number
        last_num = None
        for part in reversed(parts):
            if part.isdigit():
                last_num = int(part)
                break
        
        if last_num is None:
            last_num = float('inf')
        
        return (first_name, last_num)
    
    # Get JSON files sorted by (first_name, last_number)
    json_files_list = sorted(os.listdir(nom_dir), key=extract_name_and_last_number)
    
    # Get TXT files sorted by (first_name, last_number)
    txt_files_list = sorted([f for f in os.listdir(vi_dir) if f.endswith('.txt')], 
                             key=extract_name_and_last_number)
    
    # Check if file counts match
    json_count = len(json_files_list)
    txt_count = len(txt_files_list)
    if json_count != txt_count:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: S·ªë l∆∞·ª£ng file kh√¥ng b·∫±ng nhau. JSON: {json_count}, TXT: {txt_count}")
    
    # X·ª≠ l√Ω theo k=1 ho·∫∑c k=2
    if k == 2:
        # K=2: S·ª≠ d·ª•ng mapping file
        if not mapping_path:
            raise ValueError("k=2 y√™u c·∫ßu mapping_path (ƒë∆∞·ªùng d·∫´n file mapping.xlsx)")
        if not os.path.exists(mapping_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file mapping: {mapping_path}")
        
        # ƒê·ªçc mapping file
        df = pd.read_excel(mapping_path)
        df = df.iloc[57:].reset_index(drop=True)
        
        # Helper function ƒë·ªÉ flexible ki·ªÉm tra file t·ªìn t·∫°i
        def find_file_flexible(dir_path, target_filename):
            """
            T√¨m file flexible - support case insensitive v√† mismatch extension
            
            Args:
                dir_path: Th∆∞ m·ª•c c·∫ßn t√¨m
                target_filename: T√™n file c·∫ßn t√¨m
            
            Returns:
                Full path n·∫øu t√¨m th·∫•y, None n·∫øu kh√¥ng
            """
            if not os.path.isdir(dir_path):
                return None
            
            # Th·ª≠ ki·ªÉm tra tr·ª±c ti·∫øp tr∆∞·ªõc
            full_path = os.path.join(dir_path, target_filename)
            if os.path.exists(full_path):
                return full_path
            
            # L·∫•y base name (kh√¥ng c√≥ extension)
            target_base = os.path.splitext(target_filename)[0]
            target_ext = os.path.splitext(target_filename)[1].lower()
            
            # T√¨m file v·ªõi base name gi·ªëng nhau (ignore case, ignore extension)
            for item in os.listdir(dir_path):
                item_path = os.path.join(dir_path, item)
                if not os.path.isfile(item_path):
                    continue
                
                item_base = os.path.splitext(item)[0]
                item_ext = os.path.splitext(item)[1].lower()
                
                # Match n·∫øu base name gi·ªëng (kh√¥ng case sensitive)
                if item_base.lower() == target_base.lower():
                    # N·∫øu c√≥ extension, th·ª≠ match extension
                    if target_ext:
                        # Flexible extension matching (e.g., .json c√≥ th·ªÉ l√† .txt, etc)
                        if item_ext in ['.json', '.txt', '.jpg', '.png']:
                            return item_path
                    else:
                        return item_path
            
            return None
        
        # Preprocess v√† align theo mapping
        for lst_han, lst_qn in tqdm(zip(df["hannom"].to_list(), df["quocngu"].to_list()), desc="Preprocessing with mapping"):
            preprocess_han = []
            preprocess_qn = []
            files_han = ast.literal_eval(lst_han)
            files_qn = ast.literal_eval(lst_qn)

            # B·ªè qua mapping n·∫øu b·∫•t k·ª≥ file JSON/TXT n√†o kh√¥ng t·ªìn t·∫°i
            # S·ª≠ d·ª•ng flexible checking
            actual_han_files = []
            actual_qn_files = []
            missing_han_list = []
            missing_qn_list = []
            
            for f in files_han:
                actual_file = find_file_flexible(nom_dir, f)
                if actual_file:
                    actual_han_files.append(actual_file)
                else:
                    missing_han_list.append(f)
            
            for f in files_qn:
                actual_file = find_file_flexible(vi_dir, f)
                if actual_file:
                    actual_qn_files.append(actual_file)
                else:
                    missing_qn_list.append(f)
            
            if missing_han_list or missing_qn_list:
                print(f"‚ö†Ô∏è B·ªè qua mapping: thi·∫øu file")
                if missing_han_list:
                    print(f"   H√°n N√¥m: {missing_han_list}")
                if missing_qn_list:
                    print(f"   Qu·ªëc Ng·ªØ: {missing_qn_list}")
                print(f"   T√¨m ƒë∆∞·ª£c: H√°n={len(actual_han_files)}, QN={len(actual_qn_files)}")
                continue
            
            # X·ª≠ l√Ω t·ª´ng file H√°n N√¥m
            for file_path in actual_han_files:
                nom_data = process_nom(file_path, 1)
                file_name = os.path.basename(file_path)
                preprocess_han.append({
                    "file_name": file_name,
                    "data": nom_data, 
                    "number words": [len(box) for box in nom_data["text"]], 
                    "text": "".join(nom_data["text"])
                })
            
            # X·ª≠ l√Ω t·ª´ng file Qu·ªëc Ng·ªØ
            for file_path in actual_qn_files:
                quoc_ngu_list = process_quoc_ngu(file_path)
                preprocess_qn.extend(quoc_ngu_list)
            
            # Align
            flatten_nom = list("".join([page["text"] for page in preprocess_han]))
            aligned_hn, aligned_qn = levenshtein_align_boxes(flatten_nom, preprocess_qn, similar, trans)
            hn_remain, qn_remain = aligned_hn.copy(), aligned_qn.copy()
            
            # X·ª≠ l√Ω t·ª´ng page
            for page_idx, page_content in enumerate(preprocess_han):
                segments = []
                for num in page_content["number words"]:
                    if num == 0:
                        segments.append(("", ""))
                        continue
                    count, i = 0, 0
                    while i < len(hn_remain):
                        if hn_remain[i] != "*":
                            count += 1
                        i += 1
                        if count == num:
                            break
                    han_seg = hn_remain[:i]
                    qn_seg = qn_remain[:i]
                    segments.append((han_seg, qn_seg))
                    hn_remain = hn_remain[i:]
                    qn_remain = qn_remain[i:]
                
                # X·ª≠ l√Ω ph·∫ßn c√≤n l·∫°i: th√™m v√†o segment cu·ªëi c√πng n·∫øu c√≥
                # Ch·ªâ th√™m v√†o page cu·ªëi c√πng c·ªßa row hi·ªán t·∫°i
                if page_idx == len(preprocess_han) - 1:
                    if hn_remain or qn_remain:
                        if segments:
                            last_han, last_qn = segments[-1]
                            segments[-1] = (last_han + hn_remain, last_qn + qn_remain)
                        else:
                            segments.append((hn_remain, qn_remain))
                
                # Ghi k·∫øt qu·∫£
                nom_data = page_content["data"]
                if len(nom_data['bbox']) != len(segments):
                    print(f"‚ö†Ô∏è B·ªè qua {page_content['file_name']}: S·ªë bbox ({len(nom_data['bbox'])}) ‚â† segments ({len(segments)})")
                    continue
                
                with open(output_txt, "a", encoding="utf-8") as f:
                    for bbox, (han_seg, qn_seg) in zip(nom_data['bbox'], segments):
                        if len(han_seg) != len(qn_seg):
                            print(f"‚ö†Ô∏è Warning: Mismatch ƒë·ªô d√†i align t·∫°i file {page_content['file_name']}. H√°n={len(han_seg)}, Vi·ªát={len(qn_seg)}")
                            continue
                        nom = ''.join(han_seg).strip()
                        qn = ' '.join(qn_seg).strip()
                        
                        if not nom and not qn:
                            continue
                        
                        f.write(f"{page_content['file_name']}\t{str(bbox)}\t{nom}\t{qn}\n")
        
        return  # K=2 ƒë√£ x·ª≠ l√Ω xong
    
    # K=1: X·ª≠ l√Ω b√¨nh th∆∞·ªùng (code c≈©)
    # When NOT reverse (default): TXT is reversed (paired high-to-low)
    # When reverse=True: TXT is normal order (paired low-to-high)
    if not reverse:
        txt_files_list = list(reversed(txt_files_list))
    
    # Match files by position after sorting/reversing
    for idx, json_file in enumerate(tqdm(json_files_list, desc="Processing files", unit="file")):
        if idx >= len(txt_files_list):
            print(f"‚ö†Ô∏è C·∫£nh b√°o: S·ªë l∆∞·ª£ng file JSON v∆∞·ª£t qu√° TXT, b·ªè qua {json_file}")
            break
        
        txt_file = txt_files_list[idx]
        
        try:
            nom_data = process_nom(os.path.join(nom_dir, json_file), k)
            quoc_ngu_list = process_quoc_ngu(os.path.join(vi_dir, txt_file))
        except Exception as e:
            import traceback
            print(f"‚ùå L·ªói khi ƒë·ªçc file {json_file} ho·∫∑c {txt_file}: {e}")
            print(f"   Chi ti·∫øt: {traceback.format_exc()}")
            continue
        
        # Check if nom_data has text
        print(f"üîç DEBUG {json_file}: text={len(nom_data.get('text', []))}, bbox={len(nom_data.get('bbox', []))}, k={k}")
        if not nom_data.get('text') or not nom_data.get('bbox'):
            print(f"‚ö†Ô∏è B·ªè qua {json_file}: kh√¥ng c√≥ text ho·∫∑c bbox (text: {len(nom_data.get('text', []))}, bbox: {len(nom_data.get('bbox', []))})")
            continue
        
        # Check if quoc_ngu_list is empty
        if not quoc_ngu_list:
            print(f"‚ö†Ô∏è B·ªè qua {json_file}: kh√¥ng c√≥ text Qu·ªëc Ng·ªØ")
            continue

        segments = []
        # if k == 1:
        num_word_hn = [len(sentence) for sentence in nom_data['text']]
        flatten_nom = list("".join(nom_data['text']))
        aligned_hn, aligned_qn = levenshtein_align_boxes(flatten_nom, quoc_ngu_list, similar, trans)
        hn_remain, qn_remain = aligned_hn.copy(), aligned_qn.copy()
        for num in num_word_hn:
            if num == 0:
                segments.append(("", ""))
                continue
            count, i = 0, 0
            while i < len(hn_remain):
                if hn_remain[i] != "*":
                    count += 1
                i += 1
                if count == num:
                    break

            han_seg = hn_remain[:i]
            qn_seg = qn_remain[:i]
            segments.append((han_seg, qn_seg))
            hn_remain = hn_remain[i:]
            qn_remain = qn_remain[i:]

        if hn_remain or qn_remain:
            if segments:
                last_han, last_qn = segments[-1]
                segments[-1] = (last_han + hn_remain, last_qn + qn_remain)
            else:
                segments.append((hn_remain, qn_remain))

        with open(output_txt, "a", encoding="utf-8") as f:
            if len(nom_data['bbox']) != len(segments):
                print(f"‚ö†Ô∏è B·ªè qua {json_file}: S·ªë bbox ({len(nom_data['bbox'])}) ‚â† segments ({len(segments)})")
                continue
            for bbox, (han_seg, qn_seg) in zip(nom_data['bbox'], segments):
                if len(han_seg) != len(qn_seg):
                    print(f"‚ö†Ô∏è Warning: Mismatch ƒë·ªô d√†i align t·∫°i file {json_file}. H√°n={len(han_seg)}, Vi·ªát={len(qn_seg)}")
                    continue
                nom = ''.join(han_seg).strip()
                qn = ' '.join(qn_seg).strip()

                if not nom and not qn:
                    continue

                f.write(f"{json_file}\t{str(bbox)}\t{nom}\t{qn}\n")


# if __name__ == "__main__":
#     input_dir = r"D:\lab NLP\test\output\json\\"
#     vi_dir = r"D:\lab NLP\test\output\vi_gg"
#     output_txt = "data/result.txt"
#     k = 5
#     align(input_dir, vi_dir, output_txt,k)